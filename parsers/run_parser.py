import asyncio
import os
import random
import time
from datetime import datetime

import aiofiles
from bs4 import BeautifulSoup
from parser.core import config_parser
from parser.core.logger import setup_logger
from parser.utils.browser import block_heavy_resources, click_next_page, extract_cian_id
from parser.utils.files import save_to_file_object
from parser.utils.s3_client import upload_file_to_s3
from playwright.async_api import async_playwright
from playwright_stealth import Stealth

logger = setup_logger()


async def collect_flats_from_url(browser, flat_ids: set, url: str, file_obj: str) -> None:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö —Å –æ–¥–Ω–æ–≥–æ URL –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ jsonl —Ñ–∞–π–ª
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - browser: —ç–∫–∑–µ–º–ø–ª—è—Ä –±—Ä–∞—É–∑–µ—Ä–∞ Playwright
    - flat_ids: –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö id –∫–≤–∞—Ä—Ç–∏—Ä –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    - url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    - file_obj: –æ–±—å–µ–∫—Ç —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    context = await browser.new_context(
        user_agent=random.choice(config_parser.USER_AGENTS), extra_http_headers=config_parser.DEFAULT_HEADERS
    )
    page = await context.new_page()
    await block_heavy_resources(page)

    await page.goto(url, wait_until="domcontentloaded", timeout=20000)
    # —á–µ–∫–∞–µ–º –∫–∞–ø—á—É –∏–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
    if await page.locator(config_parser.CAPCHA_BLOCK_TEXT).count() > 0:
        logger.error(f"‚ùå –ë–õ–û–ö–ò–†–û–í–ö–ê –ò–õ–ò –ö–ê–ü–ß–ê: {url}")
        raise Exception(f"–ö–∞–ø—á–∞ –Ω–∞ {url}")
    # –≤—ã—Ç—è–≥–∏–≤–∞–µ–º –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞, —Å–∫–æ–ª—å–∫–æ –æ–±—å—è–≤–ª–µ–Ω–∏–π –Ω–∞ –æ–¥–Ω–æ–º URL –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–π—Ç
    header_locator = page.locator('div[data-name="SummaryHeader"] h5')
    if await header_locator.count() > 0:
        text = await header_locator.inner_text()
        logger.info(f"üìä {text} –Ω–∞ URL: {url}")

    # –ø–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø–æ–∫–∞ –µ—Å—Ç—å –∫–Ω–æ–ø–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    for page_num in range(config_parser.MAX_PAGES_TO_PARSE):
        # –ø–æ–ª—É—á–∞–µ–º html —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø–∞—Ä—Å–∏–º –µ–≥–æ —á–µ—Ä–µ–∑ bs4
        content = await page.content()
        soup = BeautifulSoup(content, "lxml")

        # –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –∫–≤–∞—Ä—Ç–∏—Ä–∞–º–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        cards = soup.find_all("article", {"data-name": "CardComponent"})

        logger.info(f"üîé –ö–≤–∞—Ä—Ç–∏—Ä —Å–ø–∞—Ä—Å–µ–Ω–æ - {len(flat_ids)}. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num + 1}. URL: {url}")
        # –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        for card in cards:
            try:
                link_el = card.find("a", href=True)
                if not link_el:  # –±–µ–∑ —Å—Å—ã–ª–∫–∏ –Ω–µ—Ç —Å–º—ã—Å–ª–∞ –ø–∞—Ä—Å–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫—É, —Å–∫–∏–ø
                    continue

                link = link_el["href"]
                cian_id = extract_cian_id(link)
                # —Å–∫–∏–ø, –µ—Å–ª–∏ —É–∂–µ –ø–∞—Ä—Å–∏–ª–∏ –∫–≤–∞—Ä—Ç–∏—Ä—É —Å —Ç–∞–∫–∏–º id
                if cian_id in flat_ids:
                    continue
                # —Ü–µ–Ω–∞
                price_el = card.find("span", {"data-mark": "MainPrice"})
                price_text = price_el.get_text() if price_el else None
                # –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_el = card.find("span", {"data-mark": "OfferSubtitle"}) or card.find(
                    "span", {"data-mark": "OfferTitle"}
                )
                title = title_el.get_text() if title_el else None
                # —Ñ—É–ª–ª –∞–¥—Ä–µ—Å
                geo_labels = card.find_all("a", {"data-name": "GeoLabel"})
                all_geo_texts = [g.get_text() for g in geo_labels]
                address = ", ".join(all_geo_texts)
                # –∏–Ω—Ñ–∞ –æ –º–µ—Ç—Ä–æ, –µ—Å–ª–∏ –µ—Å—Ç—å
                metro_container = card.find("div", {"data-name": "SpecialGeo"})
                metro = None
                if metro_container:
                    metro = metro_container.get_text()
                # –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—å—è–≤–ª–µ–Ω–∏—è, –µ—Å–ª–∏ –µ—Å—Ç—å
                desc_el = card.find("div", {"data-name": "Description"})
                description = desc_el.get_text(strip=True) if desc_el else None

                flat_data = {
                    "id": cian_id,
                    "link": link,
                    "title": title,
                    "price": price_text,
                    "address": address,
                    "metro": metro,
                    "parsed_at": datetime.now().isoformat(),
                    "description": description,
                }

                # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –∏ –¥–æ–±–∞–≤–ª—è–µ–º id –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
                await save_to_file_object(flat_data, file_obj)
                flat_ids.add(cian_id)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ URL: {url}. –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {page_num + 1}. {e}")
                continue
        # –∫–ª–∏–∫–∞–µ–º –ø–æ –∫–Ω–æ–ø–∫–µ "–î–∞–ª—å—à–µ" –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        if page_num + 1 < config_parser.MAX_PAGES_TO_PARSE:
            success = await click_next_page(page, "nav[data-name='Pagination'] a")
            if not success:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞—à–µ–ª –∫–Ω–æ–ø–∫—É '–î–∞–ª—å—à–µ' –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num + 1}")
                break

    await page.close()
    logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω —Å–±–æ—Ä —Å URL: {url}")
    await context.close()  # –∫–æ–≥–¥–∞ —Å–ø–∞—Ä—á–∏–ª–∏ URL, –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤–∫–ª–∞–¥–∫—É –±—Ä–∞—É–∑–µ—Ä–∞


async def main():
    os.makedirs("data", exist_ok=True)

    env_date = os.getenv("EXECUTION_DATE")
    if env_date:
        start_time_dt = datetime.strptime(env_date, "%Y-%m-%d")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É—é –¥–∞—Ç—É –∏–∑ Airflow: {env_date}")
    else:
        start_time_dt = datetime.now()
        logger.info(f"–î–∞—Ç–∞ –∏–∑ Airflow –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é —Ç–µ–∫—É—â—É—é: {start_time_dt.strftime('%Y-%m-%d')}")

    async with Stealth().use_async(async_playwright()) as p:
        start_time = time.time()  # –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        browser = await p.chromium.launch(
            headless=config_parser.HEADLESS,
            # —ç—Ç–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –¥–æ–∫–µ—Ä–µ(–±–µ–∑ –Ω–∏—Ö –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏ –∂—Ä–µ—Ç –∏ –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å)
            args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-gpu", "--disable-http-cache"],
        )
        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∫–ª–∞–¥–æ–∫, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏
        semaphore = asyncio.Semaphore(config_parser.CONCURRENT_TASKS)
        flat_ids = set()

        date_str = start_time_dt.strftime("%Y-%m-%d")
        final_local = f"data/flats_{date_str}.jsonl"
        temp_local = f"data/flats_{date_str}_temp.jsonl"

        if os.path.exists(temp_local):
            os.remove(temp_local)
        # –ø–∞—Ä—Å–∏–º –≤—Å–µ URL –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        async with aiofiles.open(temp_local, mode="a", encoding="utf-8") as f:

            async def sem_task(url):
                async with semaphore:
                    # —Ä–∞–Ω–¥–æ–º –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∂–¥–æ–≥–æ URL
                    await asyncio.sleep(random.uniform(2, 5))
                    return await collect_flats_from_url(browser, flat_ids, url, f)

            # –∑–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ –≤—Å–µ–º URL –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ —Å–µ–º–∞—Ñ–æ—Ä—É
            tasks = [sem_task(u) for u in config_parser.URLS]
            await asyncio.gather(*tasks, return_exceptions=True)

        await browser.close()
        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–ø–∞—Ä—Å–µ–Ω–æ {len(flat_ids)} –∫–≤–∞—Ä—Ç–∏—Ä –∑–∞ {round(time.time() - start_time)} —Å–µ–∫.")

        if os.path.exists(temp_local):
            os.replace(temp_local, final_local)
            # –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ, –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ S3 –∏ —É–¥–∞–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
            year = start_time_dt.strftime("%Y")
            month = start_time_dt.strftime("%m")
            day = start_time_dt.strftime("%d")

            s3_object_name = f"sales/year={year}/month={month}/day={day}/flats.jsonl"

            if upload_file_to_s3(final_local, s3_object_name):
                logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ S3: {s3_object_name}")
                if os.path.exists(final_local):
                    os.remove(final_local)
            else:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ S3. –§–∞–π–ª –æ—Å—Ç–∞–≤–ª–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ: " + final_local)


if __name__ == "__main__":
    asyncio.run(main())
