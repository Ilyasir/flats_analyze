import logging

import pendulum
from airflow import DAG
from airflow.exceptions import AirflowFailException
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from utils.datasets import RAW_DATASET_CIAN_FLATS, SILVER_DATASET_CIAN_FLATS
from utils.duckdb import get_duckdb_s3_connection

OWNER = "ilyas"
DAG_ID = "silver_from_s3_to_s3"

LAYER_SOURCE = "raw"
LAYER_TARGET = "silver"

SHORT_DESCRIPTION = (
    "DAG –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–ª–æ—è raw –≤ —Å–ª–æ–π silver, –∏–∑ jsonl –≤ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π parquet –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ S3"
)

default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    "retries": 2,
    "retry_delay": pendulum.duration(minutes=10),
}


def get_and_transform_raw_data_to_silver_s3(**context) -> dict[str, int]:
    """–û—á–∏—Å—Ç–∫–∞, –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–ª–æ—è raw –≤ silver .parquet –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ S3"""
    dt = context["data_interval_start"].in_timezone("Europe/Moscow")
    raw_s3_key = (
        f"s3://{LAYER_SOURCE}/cian/year={dt.year}/month={dt.strftime('%m')}/day={dt.strftime('%d')}/flats.jsonl"
    )
    silver_s3_key = (
        f"s3://{LAYER_TARGET}/cian/year={dt.year}/month={dt.strftime('%m')}/day={dt.strftime('%d')}/flats.parquet"
    )

    con = get_duckdb_s3_connection("s3_conn")

    raw_to_silver_query = f"""
        COPY(
        WITH raw_transformed AS (
            SELECT
                id::BIGINT as id,
                -- —É–∫–æ—Ä–∞—á–∏–≤–∞–µ–º —Å—Å—ã–ª–∫—É
                SPLIT_PART(link, '?', 1)::TEXT as link,
                title::VARCHAR as title,
                -- —Ç–∏–ø –∂–∏–ª—å—è
                CASE
                    WHEN title ILIKE '%–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã%' THEN TRUE
                    ELSE FALSE
                END as is_apartament,
                CASE
                    WHEN title ILIKE '%—Å—Ç—É–¥–∏—è%' THEN TRUE
                    ELSE FALSE
                END as is_studio,
                -- –ø–ª–æ—â–∞–¥—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞, —á–∏—Å–ª–æ –ø–µ—Ä–µ–¥ –º¬≤, –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É –ø–æ–º–µ–Ω—è–µ–º
                replace(
                    regexp_replace(
                        NULLIF(regexp_extract(title, '([\d\s]+[.,]?\d*)\s*–º¬≤', 1), ''),
                        '\s+', '', 'g'
                    ), 
                    ',', '.'
                )::NUMERIC(10, 2) AS area,
                -- –∫–æ–º–Ω–∞—Ç–Ω–æ—Å—Ç—å (0 –¥–ª—è —Å—Ç—É–¥–∏–π)
                CASE 
                    WHEN title ILIKE '%—Å—Ç—É–¥–∏—è%' THEN 0
                    ELSE NULLIF(regexp_extract(title, '^(\d+)', 1), '')::INT
                END as rooms_count,
                -- —ç—Ç–∞–∂–∏
                NULLIF(regexp_extract(title, '(\d+)/\d+\s*—ç—Ç–∞–∂', 1), '')::INT as floor,
                NULLIF(regexp_extract(title, '\d+/(\d+)\s*—ç—Ç–∞–∂', 1), '')::INT as total_floors,
                -- —Ü–µ–Ω–∞, —É–±–∏—Ä–∞–µ–º –≤–∞–ª—é—Ç—É –∏ –ø—Ä–æ–±–µ–ª—ã 
                regexp_replace(price, '[^0-9]', '', 'g')::BIGINT as price,
                address::TEXT as address,
                -- —Ä–∞–∑–±–∏–≤–∞–µ–º –∞–¥—Ä–µ—Å
                SPLIT_PART(address, ',', 1)::VARCHAR as city,
                -- –æ–∫—Ä—É–≥ —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏
                NULLIF(regexp_extract(address, '([–ê-–Ø–∞-—è]+–ê–û)', 1), '')::VARCHAR as okrug,
                -- —Ä–∞–π–æ–Ω, –¥–ª—è –Ω–æ–≤–æ–π –º–æ—Å–∫–≤—ã null, —Å–ª–∏—à–∫–æ–º –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ 
                CASE
                    WHEN okrug IN ('–ù–ê–û', '–¢–ê–û') THEN NULL
                    ELSE NULLIF(regexp_extract(address, '(—Ä-–Ω\s?[^,]+)', 1), '')::VARCHAR
                END as district,
                CASE
                    WHEN okrug IN ('–ù–ê–û', '–¢–ê–û') THEN TRUE
                    ELSE FALSE
                END as is_new_moscow,
                -- –≤—Å—è –∏–Ω—Ñ–∞ –æ –º–µ—Ç—Ä–æ
                NULLIF(regexp_extract(metro, '^(.*?)\d+\s+–º–∏–Ω—É—Ç', 1), '')::VARCHAR as metro_name,
                NULLIF(regexp_extract(metro, '(\d+)\s+–º–∏–Ω—É—Ç', 1), '')::INT as metro_min,
                CASE
                    WHEN metro LIKE '%–ø–µ—à–∫–æ–º%' THEN 'walk'
                    WHEN metro LIKE '%—Ç—Ä–∞–Ω—Å%' THEN 'transport'
                END as metro_type,
                -- –≤—Ä–µ–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
                parsed_at::TIMESTAMP as parsed_at,
                description::TEXT as description,
                -- –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∞–¥—Ä–µ—Å, —Ç–æ–∫ –¥–ª—è –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏–∏
                lower(regexp_replace(address, '[^–∞-—è–ê-–Ø0-9]', '', 'g')) as norm_address
            FROM read_json_auto('{raw_s3_key}')
        ),
        -- –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ –±–∏–∑–Ω–µ—Å –∫–ª—é—á—É (—á–∏—Å—Ç—ã–π –∞–¥—Ä–µ—Å, —ç—Ç–∞–∂–∏, –∫–æ–ª-–≤–æ –∫–æ–º–Ω–∞—Ç)
        deduplicated AS (
            SELECT *,
                ROW_NUMBER() OVER (
                    PARTITION BY norm_address, floor, total_floors, rooms_count
                    ORDER BY parsed_at DESC
                ) as row_num
            FROM raw_transformed
            WHERE area IS NOT NULL -- –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±–∏—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                AND price IS NOT NULL
                AND okrug IS NOT NULL
                AND rooms_count IS NOT NULL
                AND (district IS NOT NULL OR is_new_moscow) -- —É –Ω–æ–≤–æ–π –º–æ—Å–∫–≤—ã –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å —Ä–∞–π–Ω–æ–≤
                AND round(price / area) > 50000 -- –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Ñ–µ–π–∫–∏ (–≤—Ä—è—Ç–ª–∏ —Ü–µ–Ω–∞ –∑–∞ –º–µ—Ç—Ä —Ö–∞—Ç—ã –º–µ–Ω—å—à–µ 50–∫)
        )
        -- —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet, EXLUDE —É–±–∏—Ä–∞–µ—Ç –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        SELECT * EXCLUDE (row_num, norm_address)
        FROM deduplicated
        WHERE row_num = 1) TO '{silver_s3_key}' (FORMAT PARQUET, OVERWRITE TRUE);
    """

    try:
        logging.info(f"üíª –í—ã–ø–æ–ª–Ω—è—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é: {raw_s3_key}")
        con.execute(raw_to_silver_query)

    finally:
        con.close()

    logging.info(f"‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {silver_s3_key}")

    return {  # –∞–≤—Ç–æ–ø—É—à –∫–ª—é—á–µ–π –≤ xcoms
        "raw_s3_key": raw_s3_key,
        "silver_s3_key": silver_s3_key,
    }


def check_silver_data_quality(**context):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ silver —Å–ª–æ–µ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    # –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏–∫ –∫–ª—é—á–µ–π –∏–∑ xcoms
    keys = context["ti"].xcom_pull(task_ids="transform_to_silver")
    raw_s3_key = keys["raw_s3_key"]
    silver_s3_key = keys["silver_s3_key"]

    con = get_duckdb_s3_connection("s3_conn")

    try:
        logging.info("üíª –í—ã–ø–æ–ª–Ω—è—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö")

        dq_stats: tuple[int, int, float, float] = con.execute(
            f"""
                SELECT
                    COUNT(*) as total_rows,
                    COUNT(distinct district) as all_districts,
                    MIN(area) as min_area,
                    MAX(area) as max_area
                FROM read_parquet('{silver_s3_key}')
            """
        ).fetchone()

        raw_total_rows: int = con.execute(f"SELECT count(*) FROM read_json_auto('{raw_s3_key}')").fetchone()[0]
    finally:
        con.close()

    silver_total_rows, districts, min_area, max_area = dq_stats
    diff: int = raw_total_rows - silver_total_rows  # —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ —É–¥–∞–ª–∏–ª–æ—Å—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    percent_removed: float = (diff / raw_total_rows) * 100
    # –ø—Ä–æ–≤–µ—Ä–∫–∏
    if silver_total_rows == 0:
        raise AirflowFailException("–§–∞–π–ª –ø—É—Å—Ç–æ–π!")

    if percent_removed > 50:
        logging.error(f"‚ùå –£–¥–∞–ª–µ–Ω–æ {percent_removed:.2f}% –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.")
        raise AirflowFailException("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö —É–¥–∞–ª–µ–Ω–æ!")

    if districts > 125:
        logging.warning(f"‚ö†Ô∏è –ú–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–∞–π–æ–Ω–æ–≤ - {districts}")

    if min_area < 5:
        logging.warning(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –ø–ª–æ—â–∞–¥—å: {min_area} –º¬≤")

    if max_area > 1500:
        logging.warning(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∞—è –ø–ª–æ—â–∞–¥—å: {max_area} –º¬≤")

    logging.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
    logging.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–µ–π –∏ –º—É—Å–æ—Ä–∞: {diff} —Å—Ç—Ä–æ–∫ ({percent_removed:.2f}%).")

    return {"raw_count": raw_total_rows, "silver_count": silver_total_rows, "removed": diff}


with DAG(
    dag_id=DAG_ID,
    schedule=[RAW_DATASET_CIAN_FLATS],  # –∫–∞–∫ —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–∏—Ç—Å—è –¥–∞—Ç–∞—Å–µ—Ç raw –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è —ç—Ç–æ—Ç DAG
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=["s3", "silver"],
    description=SHORT_DESCRIPTION,
) as dag:
    start = EmptyOperator(
        task_id="start",
    )

    transform_to_silver = PythonOperator(
        task_id="transform_to_silver",
        python_callable=get_and_transform_raw_data_to_silver_s3,
    )

    check_data_quality = PythonOperator(
        task_id="check_data_quality",
        python_callable=check_silver_data_quality,
    )

    end = EmptyOperator(
        task_id="end",
        outlets=[SILVER_DATASET_CIAN_FLATS],
    )

    start >> transform_to_silver >> check_data_quality >> end
