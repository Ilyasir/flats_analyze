import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.docker.operators.docker import DockerOperator
from utils.datasets import GOLD_DATASET_HISTORY
from utils.duckdb import connect_duckdb_to_pg, connect_duckdb_to_s3

OWNER = "ilyas"
DAG_ID = "ml_train_price_model"

SHORT_DESCRIPTION = "ĞŸĞ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ catboost Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ°"

LONG_DESCRIPTION = """
### ML Training Pipeline

#### Ğ¢Ğ°ÑĞºĞ¸:
**Prepare Training Dataset**:
    - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ duckdb.
    - Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· DWH `gold.history_flats`.
    - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ĞºĞ¾Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ½ ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€ (Ğ´Ğ°Ğ¶Ğµ ÑĞ½ÑÑ‚Ñ‹Ñ… Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸).
    - Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸.
    - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ² S3 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ parquet.
**Train Model**:
    - Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Docker-ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ Ñ CatBoost.
    - ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ½Ğ° ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ· S3 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ.
    - Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (`.cbm`) Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² S3.
    - ĞŸĞ¸ÑˆĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² Airflow.
"""

default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    "retries": 2,
    "retry_delay": pendulum.duration(minutes=10),
}


def get_ml_dataset_from_pg_to_s3(**context):
    """Ğ‘ĞµÑ€ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· gold ÑĞ»Ğ¾Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ² S3 Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚"""
    dt = context["data_interval_end"].in_timezone("Europe/Moscow")
    dataset_s3_key = f"s3://ml-data/datasets/dataset_{dt.format('YYYY-MM-DD')}.parquet"
    model_s3_key = f"s3://ml-data/models/model_{dt.format('YYYY-MM-DD')}.cbm"

    con = duckdb.connect()
    connect_duckdb_to_s3(con, "s3_conn")
    connect_duckdb_to_pg(con, "pg_conn")
    try:
        logging.info(f"ğŸ’» ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ² {dataset_s3_key}")
        con.execute(
            f"""
            COPY (
                WITH base_table as (
                    select
                        -- ÑĞ°Ğ¼Ñ‹Ğµ ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸, ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ ÑĞ¾ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ°Ğ¼Ğ¸
                        row_number() OVER(
                            partition by flat_hash
                            order by effective_to desc
                        ) as row_num,
                        round((price / area)) as price_per_meter,
                        is_apartament,
                        is_studio,
                        area::DOUBLE as area,
                        rooms_count,
                        floor,
                        total_floors,
                        (floor = 1) as is_first_floor,
                        (floor = total_floors) as is_last_floor,
                        is_new_moscow,
                        okrug,
                        district,
                        -- Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¾
                        CASE
                            WHEN metro_type = 'walk' THEN metro_min
                            ELSE metro_min * 5 -- Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾, ÑƒĞ¼Ğ½Ğ½Ğ¾Ğ¶Ğ°ĞµĞ¼ Ğ½Ğ° 5 Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ¾Ğ¿Ñ€Ñ‚Ğ°
                        END as metro_min,
                        flat_hash
                    from flats_db.gold.history_flats
                    where metro_min is not null
                )
                select
                    * EXCLUDE (row_num, flat_hash),
                    -- Ğ´Ğ¾Ğ¿. Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 
                    (floor::DOUBLE / total_floors::DOUBLE) as rel_floor,
                    (area / (rooms_count + 1)) as area_per_room,
                    (total_floors > 18) as is_high_rise
                from base_table
                where row_num = 1
                -- Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ±Ñ‹Ğ» ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑĞºĞ°Ñ‡ÑƒÑ‚ Ğ½Ğ° Ñ‚ĞµÑ… Ğ¶Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
                order by flat_hash
            ) TO '{dataset_s3_key}' (FORMAT PARQUET);
            """
        )
    finally:
        con.close()
    logging.info(f"âœ… Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ² {dataset_s3_key}")

    return {
        "dataset_s3_key": dataset_s3_key,
        "model_s3_key": model_s3_key,
    }


with DAG(
    dag_id=DAG_ID,
    schedule=[GOLD_DATASET_HISTORY],
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=["ml", "gold", "s3"],
    description=SHORT_DESCRIPTION,
    doc_md=LONG_DESCRIPTION,
) as dag:
    start = EmptyOperator(
        task_id="start",
    )

    prepare_training_dataset = PythonOperator(
        task_id="prepare_training_dataset",
        python_callable=get_ml_dataset_from_pg_to_s3,
    )

    # Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğµ Ñ catboost
    train_model = DockerOperator(
        task_id="train_model",
        image="catboost_train:latest",
        container_name="catboost_train_container",
        api_version="auto",
        auto_remove="force",
        docker_url="unix://var/run/docker.sock",
        network_mode="data_network",
        mount_tmp_dir=False,
        tty=True,
        mem_limit="1g",  # Ğ´Ğ»Ñ catboost 1 Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚
        environment={
            # Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº S3 Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ°
            "S3_ACCESS_KEY": "{{ conn.s3_conn.login }}",
            "S3_SECRET_KEY": "{{ conn.s3_conn.password }}",
            "S3_ENDPOINT_URL": "{{ conn.s3_conn.extra_dejson.endpoint_url }}",
            "S3_REGION_NAME": "{{ conn.s3_conn.extra_dejson.region_name }}",
            "S3_BUCKET_NAME": "ml-data",
            # Ğ±ĞµÑ€ĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸Ğ· xcoms Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ñ‚Ğ°ÑĞºĞ¸, Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ² ĞºĞ¾Ğ½ĞµĞ¹Ğ½ĞµÑ€, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¾Ğ½ ÑĞºĞ°Ñ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            "DATASET_S3_KEY": "{{ task_instance.xcom_pull(task_ids='prepare_training_dataset')['dataset_s3_key'] }}",
            "MODEL_S3_KEY": "{{ task_instance.xcom_pull(task_ids='prepare_training_dataset')['model_s3_key'] }}",
        },
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> prepare_training_dataset >> train_model >> end
