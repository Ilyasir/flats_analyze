import logging

import pendulum
from airflow import DAG
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.utils.log.secrets_masker import mask_secret
from utils.datasets import SILVER_DATASET_CIAN_FLATS
from utils.duckdb import get_duckdb_s3_connection

OWNER = "ilyas"
DAG_ID = "gold_from_s3_to_pg"

LAYER_SOURCE = "silver"
LAYER_TARGET = "gold"

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· S3 Ğ² Postgres DWH Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ½ Ğ¿Ğ¾ SCD2."

LONG_DESCRIPTION = """
## DAG: Gold Layer Ingestion (Postgres)
Ğ­Ñ‚Ğ¾Ñ‚ DAG Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ Ğ¸Ñ… Ğ¸Ğ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° (S3)
Ğ² Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ±Ğ´ **PostgreSQL** Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸.
Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ½ Ğ½Ğ° ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ **SCD2**.

### ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹:
1. **load_from_s3_to_pg_stage**: 
    - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ **DuckDB** ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
    - Ğ§ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ `postgres` Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ `ATTACH` Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğº Ğ±Ğ°Ğ·Ğµ.
    - ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ `stage_flats` Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ÑƒĞ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Parquet Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸Ğ· S3.
    - Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ENUM: `okrug_name`, `transport_type`.
2. **merge_from_stage_to_history**:
    - Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ SQL-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ½ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ `history_flats`.
    - Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ **SCD2**.

### Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° SCD2 (Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ»ÑÑ‡):
ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ñ `price` Ğ´Ğ»Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° (`flat_hash`).
Ğ•ÑĞ»Ğ¸ Ñ†ĞµĞ½Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ°ÑÑŒ:
- ĞŸĞ¾Ğ»Ğµ `effective_to` Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°.
- ĞŸĞ¾Ğ»Ğµ `is_active` ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ `FALSE`.
- Ğ’ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ¾Ğ¹, `is_active = TRUE` Ğ¸ `effective_from = parsed_at`.
"""


default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    "retries": 2,
    "retry_delay": pendulum.duration(minutes=10),
}


def load_silver_data_from_s3_to_pg(**context) -> None:
    """ĞšĞ¾Ğ¿Ğ¸Ğ¿Ğ°ÑÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· S3 Ğ² stage Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ postgres, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ duckdb"""
    dt = context["data_interval_end"].in_timezone("Europe/Moscow")
    silver_s3_key = (
        f"s3://{LAYER_SOURCE}/cian/year={dt.year}/month={dt.strftime('%m')}/day={dt.strftime('%d')}/flats.parquet"
    )
    con = get_duckdb_s3_connection("s3_conn")
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Postgres Ğ¸Ğ· Airflow Connections
    pg_conn = BaseHook.get_connection("pg_conn")
    # ĞœĞ°ÑĞºĞ¸Ñ€ÑƒĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸Ğ½ Ğ¸ Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ»Ğ¾Ğ³Ğ°Ñ… Airflow
    mask_secret(pg_conn.login)
    mask_secret(pg_conn.password)

    try:
        logging.info(f"ğŸ’» Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· {silver_s3_key} Ğ² stage Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ")
        con.execute(
            f"""
            LOAD postgres;
            CREATE SECRET IF NOT EXISTS dwh_postgres (
                TYPE postgres,
                HOST '{pg_conn.host}',
                PORT {pg_conn.port},
                DATABASE '{pg_conn.schema}',
                USER '{pg_conn.login}',
                PASSWORD '{pg_conn.password}'
            );

            ATTACH '' AS flats_db (TYPE postgres, SECRET dwh_postgres);

            TRUNCATE TABLE flats_db.gold.stage_flats;
            
            INSERT INTO flats_db.gold.stage_flats (
                flat_hash, link, title, price, is_apartament, is_studio, area, 
                rooms_count, floor, total_floors, is_new_moscow, address, 
                city, okrug, district, metro_name, metro_min, metro_type, 
                parsed_at
            )
            SELECT 
                flat_hash, link, title, price, is_apartament, is_studio, area, 
                rooms_count, floor, total_floors, is_new_moscow, address, 
                city, okrug::flats_db.gold.okrug_name, district,
                metro_name, metro_min, metro_type::flats_db.gold.transport_type, parsed_at
            FROM read_parquet('{silver_s3_key}');
            """
        )

    finally:
        con.close()
    logging.info("âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ² stage Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ")


with DAG(
    dag_id=DAG_ID,
    schedule=[SILVER_DATASET_CIAN_FLATS],
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=["pg", "gold"],
    description=SHORT_DESCRIPTION,
    doc_md=LONG_DESCRIPTION,
) as dag:
    start = EmptyOperator(
        task_id="start",
    )

    load_from_s3_to_pg_stage = PythonOperator(
        task_id="load_from_s3_to_pg_stage", python_callable=load_silver_data_from_s3_to_pg
    )

    merge_from_stage_to_history = SQLExecuteQueryOperator(
        task_id="merge_from_stage_to_history",
        conn_id="pg_conn",
        autocommit=False,
        sql="sql/stage_to_history_scd2.sql",
        show_return_value_in_logs=True,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> load_from_s3_to_pg_stage >> merge_from_stage_to_history >> end
