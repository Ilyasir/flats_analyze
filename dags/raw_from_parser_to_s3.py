import logging
import pendulum
from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.exceptions import AirflowFailException
from utils.duckdb import get_duckdb_s3_connection

OWNER = "ilyas"
DAG_ID = "raw_from_parser_to_s3"

LAYER = "raw"

SHORT_DESCRIPTION = "DAG –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ S3, –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é duckdb"

default_args = {
    'owner': OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    'retries': 2,
    "retry_delay": pendulum.duration(hours=2),
}


def check_raw_data_quality(**context) -> dict[str, int]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ S3 —Å –ø–æ–º–æ—â—å—é duckdb"""
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ S3
    dt = context["data_interval_start"].in_timezone('Europe/Moscow')
    s3_key = f"s3://{LAYER}/cian/year={dt.year}/month={dt.strftime('%m')}/day={dt.strftime('%d')}/flats.jsonl"

    con = get_duckdb_s3_connection("s3_conn")

    logging.info("üíª –í—ã–ø–æ–ª–Ω—è—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö")
    data_quality_results: tuple[int, int, int, int, int] = con.execute(
        f"""
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT id) as unique_ids,
                COUNT(price) FILTER (WHERE price IS NOT NULL AND price != '') as valid_prices,
                COUNT(address) FILTER (WHERE address IS NOT NULL AND address != '') as valid_addresses,
                COUNT(metro) FILTER (WHERE metro IS NOT NULL AND metro != '') as valid_metro
            FROM read_json_auto('{s3_key}')
        """).fetchone()
    con.close()
    # —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    total_rows, unique_ids, valid_prices, valid_addresses, valid_metro = data_quality_results

    if total_rows == 0:
        raise AirflowFailException("–§–∞–π–ª –ø—É—Å—Ç–æ–π!")
    # –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    unique_ids_rate: float = unique_ids / total_rows
    valid_prices_rate: float = valid_prices / total_rows
    valid_addresses_rate: float = valid_addresses / total_rows
    valid_metro_rate: float = valid_metro / total_rows
    # –ø—Ä–æ–≤–µ—Ä–∫–∏
    if unique_ids_rate < 0.90:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID - {unique_ids_rate:.2%}")
        raise AirflowFailException("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è ID!")
    
    if valid_prices_rate < 0.95:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ü–µ–Ω - {valid_prices_rate:.2%}")
        raise AirflowFailException("C–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö —Ü–µ–Ω!")

    if valid_addresses_rate < 0.95:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤ - {valid_addresses_rate:.2%}")
        raise AirflowFailException("C–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –∞–¥—Ä–µ—Å–æ–≤!")

    logging.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}. "
                 f"–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤: {valid_addresses_rate:.2%}. " 
                 f"–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–æ: {valid_metro_rate:.2%}")
    
    return {"total_rows": total_rows,
            "unique_ids": unique_ids,
            "valid_prices": valid_prices,
            "valid_addresses": valid_addresses}


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 1 * * *",
    default_args=default_args,
    catchup=False,
    max_active_tasks=1,
    max_active_runs=1,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
) as dag:

    start = EmptyOperator(
        task_id="start",
    )

    run_parser = DockerOperator(
        task_id='run_parser',
        image='flats-parser:2.0',
        container_name='flats_parser_container',
        api_version='auto',
        auto_remove='force',
        docker_url="unix://var/run/docker.sock",# –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–∫–µ—Ä –Ω–∞ —Ö–æ—Å—Ç–µ –∞–∏—Ä—Ñ–ª–æ—É
        network_mode="flats_analyze_default",
        mount_tmp_dir=False,
        tty=True, # –ª–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –≤–∏–¥–Ω—ã –≤ –ª–æ–≥–∞—Ö –∞–∏—Ä—Ñ–ª–æ—É
        mem_limit='4g', # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        shm_size='1g', # –¥–ª—è —Ö—Ä–æ–º–∞ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫ —Å –ø–∞–º—è—Ç—å—é –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ
        # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ—Å—Ç—É–ø–∞ –∫ S3 —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ S3 
        environment={
            'MINIO_ACCESS_KEY': "{{ conn.s3_conn.login }}",
            'MINIO_SECRET_KEY': "{{ conn.s3_conn.password }}",
            'MINIO_ENDPOINT_URL': "{{ conn.s3_conn.extra_dejson.endpoint_url }}",
            'MINIO_BUCKET_NAME': LAYER,
            'TZ': 'Europe/Moscow',
            'EXECUTION_DATE': "{{ data_interval_start.in_timezone('Europe/Moscow').format('YYYY-MM-DD') }}",
        }
    )

    check_data_quality = PythonOperator(
        task_id='check_data_quality',
        python_callable=check_raw_data_quality,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> run_parser >> check_data_quality >> end