import logging

import pendulum
from airflow import DAG
from airflow.exceptions import AirflowFailException
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.docker.operators.docker import DockerOperator
from utils.datasets import RAW_DATASET_CIAN_FLATS
from utils.duckdb import get_duckdb_s3_connection
from utils.sql import load_sql

OWNER = "ilyas"
DAG_ID = "raw_from_parser_to_s3"

LAYER = "raw"

SHORT_DESCRIPTION = (
    "Ð¡Ð±Ð¾Ñ€ ÑÑ‹Ñ€Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ‡ÐµÑ€ÐµÐ· Docker-Ð¿Ð°Ñ€ÑÐµÑ€ Ð¸ Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð² S3, c Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ duckdb"
)

LONG_DESCRIPTION = """
## DAG: Raw Data Ingestion
Ð”Ð°Ð½Ð½Ñ‹Ð¹ DAG ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð½Ð°Ñ‡Ð°Ð»Ð¾Ð¼ Ð²ÑÐµÐ³Ð¾ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð°.
ÐžÐ½ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð·Ð° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ð²Ð½ÐµÑˆÐ½ÐµÐ³Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ° Ð¸ Ð¸Ñ… ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Data Lake.

### ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ‚Ð°ÑÐºÐ¸:
1. **run_parser**: Ð—Ð°Ð¿ÑƒÑÐº ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð° Ñ Ð¿Ð°Ñ€ÑÐµÑ€Ð¾Ð¼ Ñ‡ÐµÑ€ÐµÐ· `DockerOperator`. 
    - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ **Playwright** Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð´Ð»Ñ Ð¾Ð±Ñ…Ð¾Ð´Ð° Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð².
    - ÐšÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñƒ Ð²Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¾ 3GB RAM Ð´Ð»Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ **Playwright**.
    - Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÑ€Ð°Ð·Ñƒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑŽÑ‚ÑÑ Ð² **S3 (Minio)** Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ `.jsonl`.
    - ÐŸÐ°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð² ÑÑ‚Ð¸Ð»Ðµ Hive: `year=YYYY/month=MM/day=DD/`.
2. **check_data_quality**: Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ„Ð°Ð¹Ð»Ð° Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ **DuckDB**. 
    - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸.
    - ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ID.
    - ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾Ð»ÐµÐ¹: Ñ†ÐµÐ½Ð°, Ð°Ð´Ñ€ÐµÑ, Ð¼ÐµÑ‚Ñ€Ð¾, Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ.

### ÐžÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸:
- **Ð Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ**: Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾ Ð² 23:00 (ÐœÐ¡Ðš).
- **Ð˜Ð´ÐµÐ¼Ð¿Ð¾Ñ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ**: ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð°, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¿Ð°Ñ€ÑÐµÑ€ Ð±ÐµÑ€ÐµÑ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÑ€ÐµÐ· ÑÐ°Ð¹Ñ‚Ð°, Ð½ÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ.
ÐÐ¾ Ð·Ð°Ð¿ÑƒÑÐº Ð·Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ñ„Ð°Ð¹Ð» Ð² ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹ Ð¿Ð°Ð¿ÐºÐµ S3.
- **Ð¢Ñ€Ð¸Ð³Ð³ÐµÑ€**: ÐŸÐ¾ÑÐ»Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ `RAW_DATASET_CIAN_FLATS`.
"""


default_args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    "retries": 2,
    "retry_delay": pendulum.duration(hours=1),
}


def check_raw_data_quality(**context) -> dict[str, int]:
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² S3 Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ duckdb"""
    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ð² S3
    dt = context["data_interval_end"].in_timezone("Europe/Moscow")
    raw_s3_key = f"s3://{LAYER}/cian/year={dt.year}/month={dt.strftime('%m')}/day={dt.strftime('%d')}/flats.jsonl"

    con = get_duckdb_s3_connection("s3_conn")

    try:
        logging.info(f"ðŸ’» Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÑŽ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {raw_s3_key}")
        dq_stats: tuple[int, int, int, int, int, int] = con.execute(
            load_sql("raw_dq.sql", raw_s3_key=raw_s3_key)
        ).fetchone()

    finally:
        con.close()
    # Ñ€Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¸Ð· ÐºÐ¾Ñ€Ñ‚ÐµÐ¶Ð°
    total_rows, unique_ids, valid_prices, valid_addresses, valid_metro, valid_description = dq_stats
    # ÐµÑÐ»Ð¸ ÑÑ‚Ð¾Ðº Ð½ÐµÑ‚, ÑÑ€Ð°Ð·Ñƒ Ñ„ÐµÐ¹Ð»Ð¸Ð¼
    if total_rows == 0:
        raise AirflowFailException("Ð¤Ð°Ð¹Ð» Ð¿ÑƒÑÑ‚Ð¾Ð¹!")
    # Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑ‚ÑÐ²Ð°
    unique_ids_rate: float = unique_ids / total_rows
    valid_prices_rate: float = valid_prices / total_rows
    valid_addresses_rate: float = valid_addresses / total_rows
    valid_metro_rate: float = valid_metro / total_rows
    valid_description_rate: float = valid_description / total_rows if total_rows > 0 else 0
    # Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
    if unique_ids_rate < 0.90:
        logging.error(f"âŒ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ðµ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½Ð°. Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ID - {unique_ids_rate:.2%}")
        raise AirflowFailException("Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ…ÑÑ ID!")

    if valid_prices_rate < 0.95:
        logging.error(f"âŒ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ðµ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½Ð°. ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… Ñ†ÐµÐ½ - {valid_prices_rate:.2%}")
        raise AirflowFailException("CÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾ Ð¿ÑƒÑÑ‚Ñ‹Ñ… Ñ†ÐµÐ½!")

    if valid_addresses_rate < 0.95:
        logging.error(f"âŒ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ðµ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½Ð°. ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð°Ð´Ñ€ÐµÑÐ¾Ð² - {valid_addresses_rate:.2%}")
        raise AirflowFailException("CÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾ Ð¿ÑƒÑÑ‚Ñ‹Ñ… Ð°Ð´Ñ€ÐµÑÐ¾Ð²!")

    logging.info(
        f"âœ… ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½Ð°. Ð’ÑÐµÐ³Ð¾ ÑÑ‚Ñ€Ð¾Ðº: {total_rows}. "
        f"ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð°Ð´Ñ€ÐµÑÐ¾Ð²: {valid_addresses_rate:.2%}. "
        f"ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¾: {valid_metro_rate:.2%}. "
        f"ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹: {valid_description_rate:.2%}"
    )

    return {  # Ð¿ÑƒÑˆ Ð² xcoms, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð² UI XCom Ð²Ð¸Ð´ÐµÑ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        "total_rows": total_rows,
        "unique_ids": unique_ids,
        "valid_prices": valid_prices,
        "valid_addresses": valid_addresses,
        "valid_metro": valid_metro,
        "valid_description": valid_description,
    }


with DAG(
    dag_id=DAG_ID,
    schedule="0 23 * * *",
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
    doc_md=LONG_DESCRIPTION,
) as dag:
    start = EmptyOperator(
        task_id="start",
    )

    run_parser = DockerOperator(
        task_id="run_parser",
        image="flats-parser:2.0",  # Ð¾Ð±Ñ€Ð°Ð· Ñ Ð¿Ð°Ñ€ÑÐµÑ€Ð¾Ð¼, Ð½Ð°Ð´Ð¾ Ð·Ð°Ñ€Ð°Ð½ÐµÐµ Ð±Ð¸Ð»Ð´Ð¸Ñ‚ÑŒ
        container_name="flats_parser_container",
        api_version="auto",
        auto_remove="force",  # ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ Ð² Ð»ÑŽÐ±Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ, Ð»Ð¾Ð³Ð¸ Ð²ÑÐµ Ñ€Ð°Ð²Ð½Ð¾ Ð¿Ñ€Ð¾Ð±Ñ€Ð¾ÑˆÐµÐ½Ñ‹ Ð° Ð°Ð¸Ñ€Ñ„Ð¾Ñƒ
        docker_url="unix://var/run/docker.sock",  # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð°Ð¸Ñ€Ñ„Ð»Ð¾Ñƒ Ð¼Ð¾Ð³ Ð·Ð°Ð¿ÑƒÑÐºÐ°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ‹ Ð² Ð´Ð¾ÐºÐµÑ€Ðµ
        network_mode="data_network",  # Ð²ÑÐµ ÑÐµÑ€Ð²Ð¸ÑÑ‹ Ð² ÑÑ‚Ð¾Ð¹ ÑÐµÑ‚Ð¸ (Ð¿Ð°Ñ€ÑÐµÑ€ Ñ‚Ð¾Ð¶Ðµ), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð² minio
        mount_tmp_dir=False,  # Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¿Ð°Ð¿ÐºÐ° Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð°, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÑ€Ð°Ð·Ñƒ Ð¸Ð´ÑƒÑ‚ Ð² S3
        tty=True,  # Ð»Ð¾Ð³Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð° Ð±ÑƒÐ´ÑƒÑ‚ Ð²Ð¸Ð´Ð½Ñ‹ Ð² UI Airflow
        mem_limit="3g",  # Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°
        shm_size="1g",  # Ð´Ð»Ñ Ñ…Ñ€Ð¾Ð¼Ð° Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð±Ñ‹Ð»Ð¾ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ
        # Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº S3 Ñ‡ÐµÑ€ÐµÐ· Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð° Ð¼Ð¾Ð¶Ð½Ð¾ Ð±Ñ‹Ð»Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ‚ÑŒ Ð² S3
        environment={
            "MINIO_ACCESS_KEY": "{{ conn.s3_conn.login }}",
            "MINIO_SECRET_KEY": "{{ conn.s3_conn.password }}",
            "MINIO_ENDPOINT_URL": "{{ conn.s3_conn.extra_dejson.endpoint_url }}",
            "MINIO_BUCKET_NAME": LAYER,
            "TZ": "Europe/Moscow",
            "EXECUTION_DATE": "{{ data_interval_end.in_timezone('Europe/Moscow').format('YYYY-MM-DD') }}",
        },
    )

    check_data_quality = PythonOperator(
        task_id="check_data_quality",
        python_callable=check_raw_data_quality,
    )

    end = EmptyOperator(
        task_id="end",
        outlets=[RAW_DATASET_CIAN_FLATS],  # Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ DAG
    )

    start >> run_parser >> check_data_quality >> end
